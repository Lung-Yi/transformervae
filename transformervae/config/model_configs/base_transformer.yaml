# Base TransformerVAE Model Configuration
# A standard configuration suitable for most molecular generation tasks

encoder:
  - layer_type: "embedding"
    input_dim: 26
    output_dim: 256
    dropout: 0.0
    activation: "relu"

  - layer_type: "transformer_encoder"
    input_dim: 256
    output_dim: 256
    dropout: 0.0
    activation: "relu"
    layer_params:
      num_heads: 8
      dim_feedforward: 512
      num_layers: 4

  - layer_type: "pooling"
    input_dim: 256
    output_dim: 768  # 3 * 256 = 768 (mean + max + initial token concatenated)
    dropout: 0.0
    activation: "linear"
    layer_params:
      pooling_type: "concat_all"

sampler:
  - layer_type: "latent_sampler"
    input_dim: 768  # Changed from 256 to match pooling output
    output_dim: 256   # Latent dimension (keep small as per paper)
    dropout: 0.0
    activation: "linear"

decoder:
  - layer_type: "transformer_decoder"
    input_dim: 256  # Should match embedding dimension, not latent dimension
    output_dim: 256
    dropout: 0.0
    activation: "relu"
    layer_params:
      num_heads: 8
      dim_feedforward: 512
      num_layers: 4

  - layer_type: "linear"
    input_dim: 256
    output_dim: 26
    dropout: 0.0
    activation: "linear"